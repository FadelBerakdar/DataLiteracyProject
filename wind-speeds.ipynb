{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Analysis of Wind Speeds in the North Sea\n",
    "\n",
    "---\n",
    "\n",
    "### Data\n",
    "\n",
    "We will be working with [data](https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/10_minutes/) collected by [DWD](https://dwd.de) on Helgoland in a 10-minute intervall from 1996 to 2022. For loading the data, we have to manually collect the from a http server.\n",
    "\n",
    "\n",
    "### Imports and Usage\n",
    "\n",
    "You can simply use the `environment.yml` file to create a new conda environment with all the needed packages by running `conda env create -f environment.yml`. The only relevant parameter you may want to change is `RELOAD`. If set to `True`, all the needed data will be downloaded and saved in the `data/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import requests as rq\n",
    "import zipfile\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 400\n",
    "\n",
    "# set true to download the data and unzip into the current directory\n",
    "RELOAD = True\n",
    "ZIP_NAME = \"data.zip\"\n",
    "DATA_FOLDER = \"data\"\n",
    "\n",
    "DATA_BASE_URL = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/10_minutes/\"\n",
    "KINDS = [\"wind\", \"air_temperature\", \"precipitation\", \"solar\"]\n",
    "METRIC_URLS = [DATA_BASE_URL + postfix + \"/historical/\" for postfix in KINDS]\n",
    "print(METRIC_URLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(url: str, kind: str, station_id: str = \"02115\") -> dict:\n",
    "    response = rq.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # find dataset desciption (pdf)\n",
    "    descr_resp = rq.get(f\"{DATA_BASE_URL}/{kind}\")\n",
    "    descr_soup = BeautifulSoup(descr_resp.text, \"html.parser\")\n",
    "    descriptions = [a.get(\"href\") for a in descr_soup.find_all(\"a\", href=True) if a.get(\"href\").__contains__(\"pdf\")]\n",
    "    descrs_resps = list(map(lambda desc: rq.get(f\"{DATA_BASE_URL}/{kind}/{desc}\"), descriptions))\n",
    "\n",
    "    # Extract all links with data of the appropiate station\n",
    "    links = [a.get(\"href\") for a in soup.find_all('a', href=True) if a.get(\"href\").__contains__(station_id)]\n",
    "    print(f\"Found links: {links}\")\n",
    "\n",
    "    responses = [rq.get(url + file) for file in links]\n",
    "\n",
    "    # check if we need to create folders\n",
    "    save_path = os.path.join(\"data\", kind)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    # save dataset description pdfs\n",
    "    for descr, resp in zip(descriptions, descrs_resps):\n",
    "        with open(os.path.join(save_path, descr), \"wb\") as file:\n",
    "            file.write(resp.content)\n",
    "\n",
    "    # for each matching response download the zip and extract it\n",
    "    files = [] # remember file names of the extracted zip files\n",
    "    for i, r in enumerate(responses):\n",
    "        zip_name = os.path.join(\"data\", kind, f\"file{i}.zip\")\n",
    "        with open(zip_name, \"wb\") as z:\n",
    "            z.write(r.content)\n",
    "\n",
    "        with zipfile.ZipFile(zip_name, \"r\") as zip_file:\n",
    "            for filename in zip_file.namelist():\n",
    "                files.append(os.path.join(save_path, filename))\n",
    "            zip_file.extractall(os.path.join(\"data\", kind))\n",
    "\n",
    "        # we don't need the zip anymore\n",
    "        os.remove(zip_name)\n",
    "    \n",
    "    return files\n",
    "\n",
    "# have mapping of metric to relevant files for loading into dataframes later on\n",
    "kind_files = { kind : [] for kind in KINDS }\n",
    "kind_file_paths = os.path.join(\"data\", \"contents.pickle\")\n",
    "if RELOAD:\n",
    "    for kind, folder in zip(KINDS, METRIC_URLS):\n",
    "        print(f\"{kind}: {folder}\")\n",
    "        files = download_and_extract(folder, kind)\n",
    "        kind_files[kind] = files\n",
    "\n",
    "    with open(os.path.join(kind_file_paths), 'wb') as handle:\n",
    "        pickle.dump(kind_files, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "else:\n",
    "    with open(kind_file_paths, 'rb') as handle:\n",
    "        kind_files = pickle.load(handle)\n",
    "\n",
    "for kind in KINDS:\n",
    "    print(f\"Files containing {kind} data:\\n\\t{kind_files[kind]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind_df = { kind: None for kind in KINDS }\n",
    "for kind, files in kind_files.items():\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file, sep=\";\")\n",
    "        df[\"MESS_DATUM\"] = pd.to_datetime(df[\"MESS_DATUM\"], format=\"%Y%m%d%H%M\")\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs)\n",
    "    df.sort_values(by=\"MESS_DATUM\", inplace=True)\n",
    "    df.columns = map(lambda c: c if c == \"STATIONS_ID\" or c == \"MESS_DATUM\" else f\"{c}_{kind}\", df.columns)\n",
    "    print(f\"{kind}:\\n{df.describe()}\")\n",
    "    kind_df[kind] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = list(kind_df.values())\n",
    "df = pd.merge(dfs[0], dfs[1], on=[\"MESS_DATUM\", \"STATIONS_ID\"], how=\"inner\", suffixes=tuple(list(map(lambda x: \"_\" + x, kind_df.keys()))[:2]))\n",
    "\n",
    "# Loop through the remaining DataFrames and merge with the result\n",
    "for i, df1 in enumerate(dfs[2:]):\n",
    "    df = pd.merge(df, df1, on=[\"MESS_DATUM\", \"STATIONS_ID\"], how=\"inner\", suffixes=(None, \"_\" + list(kind_df.keys())[i+2]))\n",
    "\n",
    "# Display the result\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorative Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(12, 6))\n",
    "corr = df[[\"FF_10_wind\", \"TT_10_air_temperature\", \"PP_10_air_temperature\", \"RF_10_air_temperature\", \"TM5_10_air_temperature\", \"RWS_10_precipitation\", \"GS_10_solar\", \"SD_10_solar\"]].corr().round(2)\n",
    "sns.heatmap(corr, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind = kind_df[\"wind\"]\n",
    "wind['FF_10_wind'] = wind['FF_10_wind'].replace(-999, float('nan'))\n",
    "mask = (wind['MESS_DATUM'].dt.day == 1) & (wind['MESS_DATUM'].dt.month == 1) & (wind['MESS_DATUM'].dt.year == 2022)\n",
    "plt.plot(wind[mask][\"MESS_DATUM\"], wind[mask][\"FF_10_wind\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
